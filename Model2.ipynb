{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY0_ig6xjEqS",
        "outputId": "c21aa4b4-4f5d-4c1f-9c66-3feebf3824f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content\n",
            "/content/gdrive/MyDrive/Reto2/datasets\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")  \n",
        "!pwd  # show current path \n",
        "%cd \"/content/gdrive/MyDrive/Reto2/datasets\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de regresion logistica usando sklearn"
      ],
      "metadata": {
        "id": "dudlLmQZrO6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing, svm\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import explained_variance_score, mean_squared_error, confusion_matrix"
      ],
      "metadata": {
        "id": "Up6nCn6-jNUX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_full = pd.read_csv('trips2.csv')\n",
        "comunas_origin = dataset_full['origin']\n",
        "comunas_origin = pd.get_dummies(comunas_origin)\n",
        "dataset_full = dataset_full.join(comunas_origin)\n",
        "dataset_full = dataset_full.drop(['Unnamed: 0','lon','lat','origin'], axis=1)"
      ],
      "metadata": {
        "id": "1j6IpZa5jNn4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_x = dataset_full.loc[:, dataset_full.columns != 'destination']\n",
        "\n",
        "df_y = dataset_full[\"destination\"]\n",
        "\n",
        " ########################### separación de los datos y entrenamiento del modelo ###############################\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(df_x.values, df_y.values,random_state=1)\n",
        "model = LogisticRegression(fit_intercept = True)\n",
        "model.fit(Xtrain,ytrain)\n",
        "pred_y = model.predict(Xtest)\n",
        "acc = accuracy_score(ytest, pred_y)"
      ],
      "metadata": {
        "id": "2HyuIBO3jXYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## accuracy ########################\n",
        "print(\"accuracy of the model: \"+ str(acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERhe-BkhuYUM",
        "outputId": "e92c8dc8-7685-4a86-c2aa-a5f24594b1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of the model: 0.06548930373466055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de arbol de decision Tensorflow"
      ],
      "metadata": {
        "id": "xpgMkthirJTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow Decision Forests\n",
        "!pip install tensorflow_decision_forests"
      ],
      "metadata": {
        "id": "ceVgHIxmv-wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnas = ['destination', 'start_time', 'end_time', '0', 'ALHUÉ', 'BUIN',\n",
        "       'CALERA DE TANGO', 'CERRILLOS', 'CERRO NAVIA', 'COLINA', 'CONCHALÍ',\n",
        "       'CURACAVÍ', 'EL BOSQUE', 'EL MONTE', 'ESTACIÓN CENTRAL', 'HUECHURABA',\n",
        "       'INDEPENDENCIA', 'ISLA DE MAIPO', 'LA CISTERNA', 'LA FLORIDA',\n",
        "       'LA GRANJA', 'LA PINTANA', 'LA REINA', 'LAMPA', 'LAS CONDES',\n",
        "       'LO BARNECHEA', 'LO ESPEJO', 'LO PRADO', 'MACUL', 'MAIPÚ',\n",
        "       'MARÍA PINTO', 'MELIPILLA', 'PADRE HURTADO', 'PAINE',\n",
        "       'PEDRO AGUIRRE CERDA', 'PEÑAFLOR', 'PEÑALOLÉN', 'PIRQUE', 'PROVIDENCIA',\n",
        "       'PUDAHUEL', 'PUENTE ALTO', 'QUILICURA', 'QUINTA NORMAL', 'RECOLETA',\n",
        "       'RENCA', 'SAN BERNARDO', 'SAN JOAQUÍN', 'SAN JOSÉ DE MAIPO',\n",
        "       'SAN MIGUEL', 'SAN PEDRO', 'SAN RAMÓN', 'SANTIAGO', 'TALAGANTE',\n",
        "       'TILTIL', 'VITACURA', 'ÑUÑOA']"
      ],
      "metadata": {
        "id": "Ni33u-gu1vNH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_decision_forests as tfdf # Load TensorFlow Decision Forests\n",
        "\n",
        "# Load the training dataset using pandas\n",
        "import pandas as pd\n",
        "\n",
        "dataset_full = pd.read_csv('trips2.csv')\n",
        "dataset_full = dataset_full.sample(50000)\n",
        "comunas_origin = dataset_full['origin']\n",
        "comunas_origin = pd.get_dummies(comunas_origin)\n",
        "dataset_full = dataset_full.join(comunas_origin)\n",
        "dataset_full = dataset_full.drop(['Unnamed: 0','lon','lat','origin'], axis=1)\n",
        "\n",
        "train_df,test_df = train_test_split(dataset_full.values,random_state=42)\n",
        "\n",
        "#### como la funcion anterior nos devuelve un np array, lo regreso a datafram\n",
        "train_df = pd.DataFrame(train_df)\n",
        "test_df = pd.DataFrame(test_df)\n",
        "## les devuelvo los nombres de las columnas (se perdieron al volverse np_array)\n",
        "train_df.columns = columnas\n",
        "test_df.columns = columnas\n",
        "\n",
        "\n",
        "# Convert the pandas dataframe into a TensorFlow dataset\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"destination\")\n",
        "  \n",
        "# Train the model\n",
        "model = tfdf.keras.RandomForestModel()\n",
        "model.fit(train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le3Yib4cjY7a",
        "outputId": "d0a1ed8f-b71c-4f15-c17b-57e9610af49b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TensorFlow Decision Forests 1.0.1 is compatible with the following TensorFlow Versions: ['2.10.0']. However, TensorFlow 2.10.1 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_decision_forests/keras/core_inference.py:873: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  features_dataframe = dataframe.drop(label, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpa4d6z0fb as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:12.295129. Found 37500 examples.\n",
            "Training model...\n",
            "Model trained in 0:02:40.512698\n",
            "Compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7ff5d2189ef0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7ff5d2189ef0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Model compiled.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff633a7df90>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "dataset_full = pd.read_csv('trips2.csv')\n",
        "comunas_origin = dataset_full['origin']\n",
        "comunas_origin = pd.get_dummies(comunas_origin)\n",
        "dataset_full = dataset_full.join(comunas_origin)\n",
        "dataset_full = dataset_full.drop(['Unnamed: 0','lon','lat','origin'], axis=1)\n",
        "\n",
        "\n",
        "train_df,test_df = train_test_split(dataset_full.values,random_state=42)\n",
        "train_df = pd.DataFrame(train_df)\n",
        "train_df.columns = ['destination', 'start_time', 'end_time', '0', 'ALHUÉ', 'BUIN',\n",
        "       'CALERA DE TANGO', 'CERRILLOS', 'CERRO NAVIA', 'COLINA', 'CONCHALÍ',\n",
        "       'CURACAVÍ', 'EL BOSQUE', 'EL MONTE', 'ESTACIÓN CENTRAL', 'HUECHURABA',\n",
        "       'INDEPENDENCIA', 'ISLA DE MAIPO', 'LA CISTERNA', 'LA FLORIDA',\n",
        "       'LA GRANJA', 'LA PINTANA', 'LA REINA', 'LAMPA', 'LAS CONDES',\n",
        "       'LO BARNECHEA', 'LO ESPEJO', 'LO PRADO', 'MACUL', 'MAIPÚ',\n",
        "       'MARÍA PINTO', 'MELIPILLA', 'PADRE HURTADO', 'PAINE',\n",
        "       'PEDRO AGUIRRE CERDA', 'PEÑAFLOR', 'PEÑALOLÉN', 'PIRQUE', 'PROVIDENCIA',\n",
        "       'PUDAHUEL', 'PUENTE ALTO', 'QUILICURA', 'QUINTA NORMAL', 'RECOLETA',\n",
        "       'RENCA', 'SAN BERNARDO', 'SAN JOAQUÍN', 'SAN JOSÉ DE MAIPO',\n",
        "       'SAN MIGUEL', 'SAN PEDRO', 'SAN RAMÓN', 'SANTIAGO', 'TALAGANTE',\n",
        "       'TILTIL', 'VITACURA', 'ÑUÑOA']\n",
        "train_df\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "yMSRs_FizYtM",
        "outputId": "e3b482d3-4a48-4d93-9e82-0635b95a1719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndataset_full = pd.read_csv('trips2.csv')\\ncomunas_origin = dataset_full['origin']\\ncomunas_origin = pd.get_dummies(comunas_origin)\\ndataset_full = dataset_full.join(comunas_origin)\\ndataset_full = dataset_full.drop(['Unnamed: 0','lon','lat','origin'], axis=1)\\n\\n\\ntrain_df,test_df = train_test_split(dataset_full.values,random_state=42)\\ntrain_df = pd.DataFrame(train_df)\\ntrain_df.columns = ['destination', 'start_time', 'end_time', '0', 'ALHUÉ', 'BUIN',\\n       'CALERA DE TANGO', 'CERRILLOS', 'CERRO NAVIA', 'COLINA', 'CONCHALÍ',\\n       'CURACAVÍ', 'EL BOSQUE', 'EL MONTE', 'ESTACIÓN CENTRAL', 'HUECHURABA',\\n       'INDEPENDENCIA', 'ISLA DE MAIPO', 'LA CISTERNA', 'LA FLORIDA',\\n       'LA GRANJA', 'LA PINTANA', 'LA REINA', 'LAMPA', 'LAS CONDES',\\n       'LO BARNECHEA', 'LO ESPEJO', 'LO PRADO', 'MACUL', 'MAIPÚ',\\n       'MARÍA PINTO', 'MELIPILLA', 'PADRE HURTADO', 'PAINE',\\n       'PEDRO AGUIRRE CERDA', 'PEÑAFLOR', 'PEÑALOLÉN', 'PIRQUE', 'PROVIDENCIA',\\n       'PUDAHUEL', 'PUENTE ALTO', 'QUILICURA', 'QUINTA NORMAL', 'RECOLETA',\\n       'RENCA', 'SAN BERNARDO', 'SAN JOAQUÍN', 'SAN JOSÉ DE MAIPO',\\n       'SAN MIGUEL', 'SAN PEDRO', 'SAN RAMÓN', 'SANTIAGO', 'TALAGANTE',\\n       'TILTIL', 'VITACURA', 'ÑUÑOA']\\ntrain_df\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the testing dataset\n",
        "# Convert it to a TensorFlow dataset\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"destination\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.compile(metrics=[\"accuracy\"])\n",
        "print('accuracy: '+ str(model.evaluate(test_ds)))\n"
      ],
      "metadata": {
        "id": "ae2-UzPGpsrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b398bfde-03f6-4db9-842d-c16e030bf770"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Some of the feature names have been changed automatically to be compatible with SavedModels because fix_feature_names=True.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 3s 156ms/step - loss: 0.0000e+00 - accuracy: 0.5854\n",
            "accuracy: [0.0, 0.5853599905967712]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Ge9c8im79mW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another model with specified hyper-parameters\n",
        "model2 = tfdf.keras.GradientBoostedTreesModel(\n",
        "    num_trees=300,\n",
        "    growing_strategy=\"BEST_FIRST_GLOBAL\",\n",
        "    max_depth=5,\n",
        "    split_axis=\"SPARSE_OBLIQUE\",\n",
        "    )\n",
        "\n",
        "# Evaluate the model\n",
        "model2.compile(metrics=[\"accuracy\"])\n",
        "model2.fit(train_ds)\n",
        "print(model2.evaluate(test_ds))\n",
        "# >> 0.986851"
      ],
      "metadata": {
        "id": "blcUKk7G9P0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40a005f-3ec0-4430-ac98-3f6cf9fe747b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpg2kkx5cw as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:02.117733. Found 37500 examples.\n",
            "Training model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo con una red Neuronal"
      ],
      "metadata": {
        "id": "rpqrW0KnEjST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPool2D\n",
        "#from tf import convert_to_tensor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "EakZZKl8Ei43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "TGxCC3N_kA_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(columnas) #destination = columnas[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuY68vNhIf0e",
        "outputId": "585349a1-d04b-4487-cce9-87979eedb23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text_index(df, name):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df[name] = le.fit_transform(df[name])\n",
        "    return le.classes_"
      ],
      "metadata": {
        "id": "fNvlrYmVr9sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('trips2.csv')\n",
        "df = df.sample(50000)\n",
        "comunas_origin = df['origin']\n",
        "comunas_origin = pd.get_dummies(comunas_origin)\n",
        "df = df.join(comunas_origin)\n",
        "df = df.drop(['Unnamed: 0','lon','lat','origin'], axis=1)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df.destination)\n",
        "df['destination'] = le.transform(df.destination)\n",
        "\n",
        "df_x = df.loc[:, df.columns != 'destination']\n",
        "df_y = df[\"destination\"]\n",
        "\n",
        "#df_x = df_x.values\n",
        "#df_y = df_y.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y)\n",
        "#X_train=X_train.values.astype(np.float32)\n",
        "#y_train=y_train.values.astype(np.float32)\n",
        "#df_train = pd.DataFrame(X_train, columns=columnas)\n",
        "\n",
        "comunas = encode_text_index(df,'destination')\n",
        "\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Dense(32, input_dim=55))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(32, activation=\"relu\"))\n",
        "model3.add(Dense(53, activation=\"softmax\"))\n",
        "\n",
        "model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),#optimizer=\"adam\",                                   #Ponemos la función de optimización\n",
        "               loss=\"sparse_categorical_crossentropy\",                         #Ponemos la función de perdidas\n",
        "               metrics=[\"accuracy\"])                               #La métrica de error que evaluará lo bien que lo hace nuestra red neuronal (la precisión)\n",
        "\n",
        "model3.summary()                                                   #Imprime un resumen de nuestro modelo (no es necesario)"
      ],
      "metadata": {
        "id": "oLFXg7uWhIQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a021f50-5ecb-439d-b58d-9c146bd59fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 32)                1792      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 53)                1749      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,493\n",
            "Trainable params: 21,493\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3NeKOVaR151",
        "outputId": "685a9050-3f2f-4a44-c6ec-8f7fc6096939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['start_time', 'end_time', '0', 'ALHUÉ', 'BUIN', 'CALERA DE TANGO',\n",
              "       'CERRILLOS', 'CERRO NAVIA', 'COLINA', 'CONCHALÍ', 'CURACAVÍ',\n",
              "       'EL BOSQUE', 'EL MONTE', 'ESTACIÓN CENTRAL', 'HUECHURABA',\n",
              "       'INDEPENDENCIA', 'ISLA DE MAIPO', 'LA CISTERNA', 'LA FLORIDA',\n",
              "       'LA GRANJA', 'LA PINTANA', 'LA REINA', 'LAMPA', 'LAS CONDES',\n",
              "       'LO BARNECHEA', 'LO ESPEJO', 'LO PRADO', 'MACUL', 'MAIPÚ',\n",
              "       'MARÍA PINTO', 'MELIPILLA', 'PADRE HURTADO', 'PAINE',\n",
              "       'PEDRO AGUIRRE CERDA', 'PEÑAFLOR', 'PEÑALOLÉN', 'PIRQUE', 'PROVIDENCIA',\n",
              "       'PUDAHUEL', 'PUENTE ALTO', 'QUILICURA', 'QUINTA NORMAL', 'RECOLETA',\n",
              "       'RENCA', 'SAN BERNARDO', 'SAN JOAQUÍN', 'SAN JOSÉ DE MAIPO',\n",
              "       'SAN MIGUEL', 'SAN PEDRO', 'SAN RAMÓN', 'SANTIAGO', 'TALAGANTE',\n",
              "       'TILTIL', 'VITACURA', 'ÑUÑOA'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(comunas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fwva1VrshQV",
        "outputId": "338c73f1-39c7-4995-8d21-a275b9a193f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train = np.array(X_train)\n",
        "#y_train = np.array(y_train)\n",
        "\n",
        "#X_train = X_train.values.tolist()\n",
        "#y_train = y_train.values.tolist()\n",
        "\n",
        "#X_train = tf.convert_to_tensor(X_train)\n",
        "#y_train = tf.convert_to_tensor(y_train)"
      ],
      "metadata": {
        "id": "6NEU1BGrNWVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train= np.array(X_train)\n",
        "X_train = X_train.values\n",
        "y_train = y_train.values\n",
        "type(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QCNMBXdRujF",
        "outputId": "d163d301-a6e3-4c49-89ed-d18a404540ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model3.fit(X_train, y_train,                                      #Entrenamiento del modelo, pasandole las variables (betas) y el target a predecir (y)\n",
        "           #validation_data = (X_test,y_test),\n",
        "           epochs=20)                                                     #El número de epocas, una epoca es cuando el modelo en pasa por todos los datos y actualizar las betas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B18oIzvKJA77",
        "outputId": "9cf47841-4889-42a9-f444-c5677dd6fdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1172/1172 [==============================] - 12s 9ms/step - loss: 3.7656 - accuracy: 0.0613\n",
            "Epoch 2/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6933 - accuracy: 0.0618\n",
            "Epoch 3/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6928 - accuracy: 0.0613\n",
            "Epoch 4/20\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 3.6923 - accuracy: 0.0619\n",
            "Epoch 5/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6921 - accuracy: 0.0635\n",
            "Epoch 6/20\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 3.6918 - accuracy: 0.0621\n",
            "Epoch 7/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6914 - accuracy: 0.0610\n",
            "Epoch 8/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6914 - accuracy: 0.0603\n",
            "Epoch 9/20\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 3.6912 - accuracy: 0.0606\n",
            "Epoch 10/20\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 3.6908 - accuracy: 0.0623\n",
            "Epoch 11/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6911 - accuracy: 0.0605\n",
            "Epoch 12/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6907 - accuracy: 0.0603\n",
            "Epoch 13/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6907 - accuracy: 0.0625\n",
            "Epoch 14/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6904 - accuracy: 0.0623\n",
            "Epoch 15/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6905 - accuracy: 0.0594\n",
            "Epoch 16/20\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 3.6903 - accuracy: 0.0624\n",
            "Epoch 17/20\n",
            "1172/1172 [==============================] - 5s 5ms/step - loss: 3.6905 - accuracy: 0.0613\n",
            "Epoch 18/20\n",
            "1172/1172 [==============================] - 5s 5ms/step - loss: 3.6905 - accuracy: 0.0624\n",
            "Epoch 19/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6901 - accuracy: 0.0607\n",
            "Epoch 20/20\n",
            "1172/1172 [==============================] - 5s 4ms/step - loss: 3.6904 - accuracy: 0.0623\n"
          ]
        }
      ]
    }
  ]
}